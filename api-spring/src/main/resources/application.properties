spring.application.name=spring-ai-demo
logging.level.com.wininger.spring_ai_demo=DEBUG


spring.ai.ollama.base-url=http://192.168.1.23:11434
#spring.ai.ollama.chat.options.model=deepseek-r1-19152:8b
# this will increase memory usage quite a bit, but looks
# like changing it really does increase the context window, no
# need for custom ollama builds anymore
#spring.ai.ollama.chat.options.num-ctx=32768
#spring.ai.ollama.chat.options.num-ctx=19152
spring.ai.ollama.chat.options.model=deepseek-r1:8b-0528-qwen3-q4_K_M
#spring.ai.ollama.chat.options.model=gemma3:4b

# hypothetically, most of these models can go up to 131K
spring.ai.ollama.chat.options.num-ctx=131072

# keep it predictable so that we can very only the rag
spring.ai.ollama.embedding.options.seed=1

# disable this for gemma
spring.ai.ollama.chat.options.think-option=enabled

#spring.ai.ollama.chat.options.model=deepseek-r1-32768:7b
#spring.ai.ollama.chat.options.model=deepseek-r1:1.5b

spring.datasource.url=jdbc:postgresql://localhost:5436/spring-ai-demo-db
spring.datasource.username=postgres
spring.datasource.password=xxx
spring.datasource.driver-class-name=org.postgresql.Driver

spring.ai.vectorstore.pgvector.index-type=HNSW
spring.ai.vectorstore.pgvector.distance-type=COSINE_DISTANCE

# needs to match what we set when creating the vector table
# also should be based on the embedding used which (mxbai-embed-large:latest by default)
# according to `ollama show mxbai-embed-large:latest`, embedding length is 1024
spring.ai.vectorstore.pgvector.dimensions=1024


#spring.ai.ollama.embedding.options.model=nomic-embed-text:latest
#spring.ai.vectorstore.pgvector.dimensions=768

# not sure if this is needed or indeed doing anything
spring.ai.ollama.embedding.options.max-tokens=16000
#spring.ai.embedding.transformer.batch.single-document-max-token-count=8000
#spring.ai.embedding.transformer.batch.max-token-count=16000

# OpenAPI/Swagger documentation
springdoc.api-docs.path=/api-docs
springdoc.swagger-ui.path=/swagger-ui.html

